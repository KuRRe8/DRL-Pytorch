# 强化学习

虽然已经使用过，并且看过3次huggingface的强化学习教程，但过段时间印象又会模糊。这次根据这个仓库内容好好复习一下做一些笔记。

## Q-Learning

最基础的强化学习，实际上只需要迭代训练一个Q表，行是不同的状态s，列是不同的动作a，对应的格子就是Q值，值越大越应该选，最开始初始化为0。
仿真环境会根据已有内部保存的当前时刻状态s，代码选择的a，给出s next， 和反馈reward。在实际环境中，需要有一些启发式或者确定的reward。
贝尔曼方程更新Q：
Q(s, a) ← Q(s, a) + lr * [r + γ * max_a' Q(s_next, a') - Q(s, a)]
也就是假设下一步按最大Q来选，这个Q打个γ折扣，加上当前奖励，算作这次计算的Q'值，当前我们迭代优化时候一般都要按学习率lr来递增，而不是一下就改成Q'
这样，每次和环境交互，都能更新一下Q表，时间足够久以后，Q表就合理了。

在每次做出决策时候可以按epsilon（0.1）的概率来决定是随机action还是确定使用最大Q的action，这样做到了既往经验和新探索的平衡。

## DQN系列

## Duel Double DQN